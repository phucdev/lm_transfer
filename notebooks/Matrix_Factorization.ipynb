{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68c97a0-57fc-4ff7-bb36-03a8f9d0491d",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "In \"UNKs Everywhere: Adapting Multilingual Language Models to New Scripts\" by Pfeiffer et al. (2021) they describe a method factorize the embedding matrix **X** of a pre-trained language model into two smaller matrices:\n",
    "\n",
    "- **F**: This matrix represents new, lower-dimensional word embeddings (D' = 100). It contains token-specific information.\n",
    "- **G**: This represents up-projection matrices that help in reconstructing the original embeddings from **F**. This encodes general properties of the original embedding matrix.\n",
    "  - In the most basic case **X** is approximately **FG**, which means that **G** is shared across all tokens and encodes general properties of the original embedding matrix whereas **F** stores token-specific information. **G** only needs to be pretrained once and can be used and fine-tuned for every new language. Only need to learn new low-dimensional embeddings **F** with pretraining task.\n",
    "  - C up-projection matrices: group tokens and use separate up-projection matrix per group to balance sharing information across typologically similar languages with learning a robust representation for each token\n",
    "    - K-MEANS to cluster X into C clusters, factorize subset of embeddings associated with c-th cluster separately. For new language randomly initialize a matrix in order to compute cluster assignment matrix with softmax estimation\n",
    "    - NEURAL learns cluster assignment and up-projections jointly by parameterizing **G**\n",
    "\n",
    "When adapting to a new language, instead of starting from scratch, the method initializes the new embedding matrix using these lower-dimensional embeddings and up-projections. \n",
    "By doing so, it uses the general linguistic knowledge already present in the original model (encoded in **G**) and combines it with the specifics of the target language (captured during the adaptation process in **F**).\n",
    "The lower-dimensional word embedding **F'** of the new model is either randomly initialized or partially initialized using the embeddings in **F** of overlapping tokens between the source and the target language.\n",
    "\n",
    "For cross-lingual transfer only language adapters and embeddings **F** are trained jointly while the rest of the model (including **G**) is fixed. This further reduces the number of trainable parameters.\n",
    "\n",
    "They formulate this as a semi non-negative matrix factorization problem.\n",
    "They reference this code base: https://github.com/cthurau/pymf/tree/master/pymf\n",
    "\n",
    "That code base is quite old. We copied the relevant parts of that code base and updated it to work with our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower-dimensional word embeddings F shape: (3000, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ..src.utils.snmf import SNMF\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "vocab_size, original_dim = 3000, 768  # Example dimensions\n",
    "x = np.random.randn(vocab_size, original_dim)  # Simulated embedding matrix\n",
    "keep_dim = 100\n",
    "snmf_mdl = SNMF(x, niter=3000, num_bases=keep_dim)\n",
    "# Factorizes into \n",
    "# W - low-dimensional word embeddings F (3000 x 100) and \n",
    "# H - up-projections G (100 x 768)\n",
    "snmf_mdl.factorize()\n",
    "f = snmf_mdl.W\n",
    "g = snmf_mdl.H\n",
    "print(f\"Lower-dimensional word embeddings F shape: {f.shape}\")\n",
    "# TODO K-means clustering/neural clustering for word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edc14fe-6ff2-4dbd-994f-b0d025a9b5c7",
   "metadata": {},
   "source": [
    "Whereas Pfeiffer et al. (2021) formulate it as semi non-negative matrix factorization, in \"OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining\" Liu et al. (2024) use Singular Value Decomposition (SVD) to factorize the word embedding matrix into a lower-dimensional (lower coordinates) matrix and an orthogonal up-projection matrix (primitive embeddings, language agnostic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0765ea8-ea72-4e04-960b-62bc9c3fb718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower-dimensional word embeddings shape: (3000, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example Usage\n",
    "vocab_size, original_dim = 3000, 768  # Example dimensions\n",
    "keep_dim = 100\n",
    "x = np.random.randn(vocab_size, original_dim)  # Simulated embedding matrix\n",
    "\n",
    "try:\n",
    "    # factorize the multilingual embedding using svd\n",
    "    u, s, vh = np.linalg.svd(x, full_matrices=True)\n",
    "\n",
    "    primitive_embeddings = np.matmul(vh.T[:, :keep_dim], np.diag(s[:keep_dim])).T\n",
    "    # primitive_embeddings size: (keep_dim, vector_size)\n",
    "\n",
    "    lower_coordinates = u[:, :keep_dim]\n",
    "    # size: (num_words, keep_dim)\n",
    "    print(f\"Lower-dimensional word embeddings shape: {lower_coordinates.shape}\")\n",
    "except:\n",
    "    raise ValueError(\"Cannot perform the factorization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27122f40-5aba-4740-9f2f-b08a60a4781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primitive embeddings shape: (100, 768)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Primitive embeddings shape: {primitive_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3710f2-406a-444c-9455-08a86a243390",
   "metadata": {},
   "source": [
    "My questions:\n",
    "\n",
    "- U and V transposed were orthogonal (orthonormal) matrices with the left/right singular vectors. Do their reduced versions retain their properties?\n",
    "- Is the product of reduced Sigma and reduced V transposed orthogonal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff103066-4678-4001-b892-a06f92d86490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  3.89228580e-17, -3.98986399e-17, ...,\n",
       "        -2.25514052e-17,  1.90819582e-17,  1.38777878e-17],\n",
       "       [ 3.89228580e-17,  1.00000000e+00, -1.76508114e-16, ...,\n",
       "        -2.08166817e-17, -2.34187669e-17, -1.21430643e-17],\n",
       "       [-3.98986399e-17, -1.76508114e-16,  1.00000000e+00, ...,\n",
       "        -5.20417043e-18, -6.93889390e-18, -2.94902991e-17],\n",
       "       ...,\n",
       "       [-2.25514052e-17, -2.08166817e-17, -5.20417043e-18, ...,\n",
       "         1.00000000e+00,  1.04083409e-17,  1.04083409e-17],\n",
       "       [ 1.90819582e-17, -2.34187669e-17, -6.93889390e-18, ...,\n",
       "         1.04083409e-17,  1.00000000e+00, -1.04083409e-17],\n",
       "       [ 1.38777878e-17, -1.21430643e-17, -2.94902991e-17, ...,\n",
       "         1.04083409e-17, -1.04083409e-17,  1.00000000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(u.T, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ad1a4-ec4c-42d1-88a1-979cc7c291c5",
   "metadata": {},
   "source": [
    "In order to to understand the differences of the matrix factorization methods, we will take a look at the example in Dinh et al. (2008)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffd24183-b942-46f1-a302-bc6d03645e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.matrix([\n",
    "    [1.3, 1.8, 4.8, 7.1, 5.0, 5.2, 8.0],\n",
    "    [1.5, 6.9, 3.9, -5.5, -8.5, -3.9, -5.5],\n",
    "    [6.5, 1.6, 8.2, -7.2, -8.7, -7.9, -5.2],\n",
    "    [3.8, 8.3, 4.7, 6.4, 7.5, 3.2, 7.4],\n",
    "    [-7.3, -1.8, -2.1, 2.7, 6.8, 4.8, 6.2],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb9bce3f-9fcd-4d0c-acd3-2517f3aee8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.3,  1.8,  4.8,  7.1,  5. ,  5.2,  8. ],\n",
       "        [ 1.5,  6.9,  3.9, -5.5, -8.5, -3.9, -5.5],\n",
       "        [ 6.5,  1.6,  8.2, -7.2, -8.7, -7.9, -5.2],\n",
       "        [ 3.8,  8.3,  4.7,  6.4,  7.5,  3.2,  7.4],\n",
       "        [-7.3, -1.8, -2.1,  2.7,  6.8,  4.8,  6.2]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94bda20e-fd7e-4db8-bf1e-4515fcc7053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, v = np.linalg.svd(data_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89738517-b395-42dc-9bb1-96a48701ef08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.19570959, -0.46558784, -0.49758201,  0.58923896,  0.00263719,\n",
       "          0.34827514,  0.1698022 ],\n",
       "        [ 0.03485142, -0.57554339,  0.78543451,  0.19257571, -0.09009707,\n",
       "         -0.02563702,  0.06904831],\n",
       "        [ 0.13145959, -0.60549496, -0.26247475, -0.59310945,  0.03866713,\n",
       "         -0.0413909 , -0.43842224],\n",
       "        [-0.45364988, -0.16907824, -0.1331825 ,  0.31560236,  0.48312969,\n",
       "         -0.62687319, -0.14794463],\n",
       "        [-0.57682319, -0.02287634, -0.07311434,  0.17287702, -0.67945481,\n",
       "          0.09704743, -0.40055269],\n",
       "        [-0.40581001,  0.0346385 ,  0.15872702, -0.05175672,  0.53645674,\n",
       "          0.6880708 , -0.21217472],\n",
       "        [-0.48989441, -0.2346125 , -0.13540392, -0.36299195, -0.08672618,\n",
       "          0.02245745,  0.73948448]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fc17e12-5c67-4ee3-861d-7fead744beaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.4480211 , 16.96105118,  6.75969418,  5.16119986,  3.28045566])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a30ed92d-4ae0-4ef3-8ceb-029e3c74e796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.39321668,  0.44719741,  0.57803086, -0.36918282, -0.41830549],\n",
       "        [-0.44568199, -0.28013616, -0.38615063, -0.72348304,  0.22439091],\n",
       "        [-0.30503919,  0.75878217, -0.45633945,  0.22188508,  0.27151801],\n",
       "        [-0.34917987, -0.2145735 , -0.42727545,  0.29344783, -0.75057025],\n",
       "        [ 0.65709414,  0.31582597, -0.35490563, -0.45269386, -0.3709333 ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee52c846-e007-4165-a2f4-5765ad46282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "primitive_embeddings = np.matmul(v.T[:, :2], np.diag(s[:2])).T\n",
    "\n",
    "lower_coordinates = u[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5693f94-fe0c-489b-bd8d-8b60f272a53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-11.18623642,  12.7218814 ,  16.4438342 , -10.50252056,\n",
       "         -11.8999635 ],\n",
       "        [ -7.55923509,  -4.7514037 ,  -6.54952068, -12.27103279,\n",
       "           3.80590567]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primitive_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5eb3b96f-eaa5-48df-bbb2-37bf94522cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.19570959, -0.46558784],\n",
       "        [ 0.03485142, -0.57554339],\n",
       "        [ 0.13145959, -0.60549496],\n",
       "        [-0.45364988, -0.16907824],\n",
       "        [-0.57682319, -0.02287634],\n",
       "        [-0.40581001,  0.0346385 ],\n",
       "        [-0.48989441, -0.2346125 ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd003c9-4f0c-4a0e-9fa3-31fc1959c47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
